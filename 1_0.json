{
  "models": [
    {
  "name": "Gemma-3-270M-IT",
  "modelId": "google/gemma-3-270m-it",
  "modelFile": "gemma-3-270m-it.safetensors",
  "description": "Compact 270-million-parameter variant of Gemma 3, instruction-tuned (IT). Designed for on-device use with efficient inference, supporting multilingual text and (potentially) image input with up to 32K token context.",
  "sizeInBytes": 536000000,
  "estimatedPeakMemoryInBytes": 550000000,
  "commitHash": "23cf460",
  "llmSupportImage": false,
  "defaultConfig": {
    "topK": 50,
    "topP": 0.95,
    "temperature": 0.8,
    "maxTokens": 2048,
    "accelerators": "cpu"
  },
  "taskTypes": [
    "llm_chat"
  ]
},
    {
      "name": "Gemma-3n-E2B-it-int4",
      "modelId": "google/gemma-3n-E2B-it-litert-preview",
      "modelFile": "gemma-3n-E2B-it-int4.task",
      "description": "Preview version of [Gemma 3n E2B](https://ai.google.dev/gemma/docs/gemma-3n) ready for deployment on Android using the [MediaPipe LLM Inference API](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference). The current checkpoint only supports text and vision input, with 4096 context length.",
      "sizeInBytes": 3136226711,
      "estimatedPeakMemoryInBytes": 5905580032,
      "commitHash": "b2b54222ba849ee74ac9f88d6af2470b390afa9e",
      "llmSupportImage": true,
      "defaultConfig": {
        "topK": 64,
        "topP": 0.95,
        "temperature": 1,
        "maxTokens": 4096,
        "accelerators": "cpu,gpu"
      },
      "taskTypes": [
        "llm_chat",
        "llm_prompt_lab",
        "llm_ask_image"
      ],
      "bestForTaskTypes": [
        "llm_ask_image",
        "llm_ask_audio"
      ]
    },
    {
      "name": "Gemma-3n-E4B-it-int4",
      "modelId": "google/gemma-3n-E4B-it-litert-preview",
      "modelFile": "gemma-3n-E4B-it-int4.task",
      "description": "Preview version of [Gemma 3n E4B](https://ai.google.dev/gemma/docs/gemma-3n) ready for deployment on Android using the [MediaPipe LLM Inference API](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference). The current checkpoint only supports text and vision input, with 4096 context length.",
      "sizeInBytes": 4405655031,
      "estimatedPeakMemoryInBytes": 6979321856,
      "commitHash": "1bb9b9c34cfe96536b5540699ffa07060e9fa124",
      "llmSupportImage": true,
      "defaultConfig": {
        "topK": 64,
        "topP": 0.95,
        "temperature": 1,
        "maxTokens": 4096,
        "accelerators": "cpu,gpu"
      },
      "taskTypes": [
        "llm_chat",
        "llm_prompt_lab",
        "llm_ask_image"
      ]
    },
    {
      "name": "Gemma3-1B-IT q4",
      "modelId": "litert-community/Gemma3-1B-IT",
      "modelFile": "Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048.task",
      "description": "A variant of [google/Gemma-3-1B-IT](https://huggingface.co/google/Gemma-3-1B-IT) with 4-bit quantization ready for deployment on Android using the [MediaPipe LLM Inference API](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference)",
      "sizeInBytes": 554661246,
      "estimatedPeakMemoryInBytes": 2147483648,
      "commitHash": "a1162a5674cd20452540a007dbe8d54e046d5f8b",
      "defaultConfig": {
        "topK": 64,
        "topP": 0.95,
        "temperature": 1,
        "maxTokens": 1024,
        "accelerators": "gpu,cpu"
      },
      "taskTypes": [
        "llm_chat",
        "llm_prompt_lab"
      ],
      "bestForTaskTypes": [
        "llm_chat",
        "llm_prompt_lab"
      ]
    }
  ]
}
